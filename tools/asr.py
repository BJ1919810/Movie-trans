#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys

import argparse
import json
import time
import traceback
from pathlib import Path
import torch
from faster_whisper import WhisperModel
from funasr import AutoModel
from modelscope import snapshot_download
from tqdm import tqdm

# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆtoolsç›®å½•çš„çˆ¶ç›®å½•ï¼‰
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# è®¾ç½®æ¨¡å‹è·¯å¾„ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
MODEL_DIR = os.path.join(project_root, "asr", "models")
os.environ["HF_HOME"] = MODEL_DIR

# è§£å†³Xetå­˜å‚¨åç«¯é—®é¢˜çš„ç¯å¢ƒå˜é‡è®¾ç½®
os.environ["HF_HUB_DISABLE_XET"] = "1"
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# FunASRæ¨¡å‹ç¼“å­˜
funasr_models = {}


def create_funasr_model(language="zh"):
    """åˆ›å»º FunASR æ¨¡å‹ç”¨äºä¸­æ–‡è¯†åˆ«ï¼Œæ”¯æŒæœ¬åœ°ç¼ºå¤±æ—¶è‡ªåŠ¨ä¸‹è½½è‡³ MODEL_DIR"""
    # å®šä¹‰æ¨¡å‹ ID ä¸æœ¬åœ°è·¯å¾„æ˜ å°„
    model_configs = {
        "zh": {
            "asr": {
                "model_id": "iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
                "local_name": "speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch"
            },
            "vad": {
                "model_id": "iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                "local_name": "speech_fsmn_vad_zh-cn-16k-common-pytorch"
            },
            "punc": {
                "model_id": "iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
                "local_name": "punc_ct-transformer_zh-cn-common-vocab272727-pytorch"
            }
        }
    }

    if language not in model_configs:
        raise ValueError(f"FunASR ä¸æ”¯æŒè¯¥è¯­è¨€: {language}")

    config = model_configs[language]
    revision = "v2.0.4"

    # æ„å»ºæœ¬åœ°è·¯å¾„ & ç¡®ä¿ MODEL_DIR å­˜åœ¨
    os.makedirs(MODEL_DIR, exist_ok=True)

    def ensure_model_downloaded(model_key, model_info):
        local_path = os.path.join(MODEL_DIR, model_info["local_name"])
        if not os.path.exists(local_path):
            print(f"[Downloading] {model_key.upper()} Model -> {local_path}")
            # æ·»åŠ é‡è¯•æœºåˆ¶
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    snapshot_download(
                        model_info["model_id"],
                        revision=revision,
                        cache_dir=MODEL_DIR,
                        local_files_only=False
                    )
                    # snapshot_download é»˜è®¤ä¼šå»º `MODEL_DIR/model_id/...`ï¼Œä½† FunASR æœŸæœ›ç›´æ¥æ˜¯æ¨¡å‹ç›®å½•
                    # å®é™…ä¸‹è½½åè·¯å¾„ä¸ºï¼šMODEL_DIR/model_id/ â†’ æˆ‘ä»¬ rename ä¸ºæœŸæœ›çš„ local_name
                    downloaded_dir = os.path.join(MODEL_DIR, model_info["model_id"].replace("/", "--"))
                    if os.path.exists(downloaded_dir):
                        os.rename(downloaded_dir, local_path)
                    else:
                        # fallback: å¯èƒ½å›  modelscope ç‰ˆæœ¬å·®å¼‚ç›´æ¥ä¸‹åˆ° local_nameï¼Ÿ
                        pass
                    print(f"[Success] {model_key.upper()} Model Download Completed <- {local_path}")
                    break
                except Exception as e:
                    print(f"[Warning] Attempt {attempt + 1} to download {model_key.upper()} model failed: {str(e)}")
                    if attempt < max_retries - 1:
                        print(f"[Retry] Performing attempt {attempt + 2}...")
                        time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
                    else:
                        raise RuntimeError(
                            f"âŒ {model_key.upper()} æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼è¯·æ£€æŸ¥ç½‘ç»œæˆ– ModelScope Tokenã€‚\n"
                            f"Model ID: {model_info['model_id']}, Revision: {revision}\n"
                            f"Error: {e}"
                        )
        else:
            print(f"[Ready] {model_key.upper()} Model <- {local_path}")
        return local_path

    # æ£€æŸ¥å¹¶ä¸‹è½½ä¸‰æ¨¡å‹
    path_asr = ensure_model_downloaded("asr", config["asr"])
    path_vad = ensure_model_downloaded("vad", config["vad"])
    path_punc = ensure_model_downloaded("punc", config["punc"])

    # ä»ç¼“å­˜åŠ è½½ or æ–°å»º
    if language in funasr_models:
        print(f"[Reuse] FunASR Model Already Loaded: {language.upper()}")
        return funasr_models[language]
    else:
        model = AutoModel(
            model=path_asr,
            model_revision=revision,
            vad_model=path_vad,
            vad_model_revision=revision,
            punc_model=path_punc,
            punc_model_revision=revision,
        )
        print(f"[Complete] FunASR Model Successfully Loaded: {language.upper()}")

        funasr_models[language] = model
        return model


def transcribe_with_funasr(audio_file, language="zh"):
    """ä½¿ç”¨FunASRè¿›è¡Œä¸­æ–‡è¯­éŸ³è¯†åˆ«"""
    try:
        model = create_funasr_model(language)
        result = model.generate(input=audio_file)
        return result[0]["text"] if result else ""
    except Exception as e:
        print(f"FunASR recognition error: {e}")
        traceback.print_exc()
        return ""


def transcribe_with_faster_whisper(audio_file, model, language=None):
    """ä½¿ç”¨Faster-Whisperè¿›è¡Œå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«"""
    try:
        if language == "zh":
            print("User specified Chinese text, processed by FunASR")
            text = transcribe_with_funasr(audio_file, language="zh")
        else:
            segments, info = model.transcribe(
                audio=audio_file,
                beam_size=5,
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=700),
                language=language,
            )
            text = ""
            # è‹¥æ£€æµ‹åˆ°çš„è¯­è¨€æ˜¯ä¸­æ–‡ï¼Œåˆ™ä½¿ç”¨FunASRè¿›è¡Œè¯†åˆ«
            if info.language == "zh" and language == None:
                print("Detected Chinese text, switching to FunASR processing")
                text = transcribe_with_funasr(audio_file, language="zh")
            # å¦‚æœFunASRæ²¡æœ‰è¿”å›ç»“æœæˆ–å…¶ä»–è¯­è¨€ï¼Œä½¿ç”¨Faster-Whisper
            else:
                for segment in segments:
                    text += segment.text
                
        return text
    except Exception as e:
        print(f"Faster-Whisper recognition error: {e}")
        traceback.print_exc()
        return ""


def get_clip_info_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå–ç‰‡æ®µä¿¡æ¯"""
    # æ–‡ä»¶åæ ¼å¼: clip_00_001_14.73-15.29.wav
    parts = filename.split("_")
    # å°†clip_00_001è½¬æ¢ä¸ºSPEAKER_00
    speaker_id = parts[1]  # 00
    speaker = f"SPEAKER_{speaker_id}"
    
    time_range = parts[3].replace(".wav", "")
    start_time, end_time = time_range.split("-")
    
    return {
        "speaker": speaker,
        "start": float(start_time),
        "end": float(end_time)
    }


def find_matching_segment(segments, clip_info):
    """åœ¨segmentsä¸­æŸ¥æ‰¾åŒ¹é…çš„ç‰‡æ®µ"""
    for segment in segments:
        # æ£€æŸ¥è¯´è¯äººæ˜¯å¦åŒ¹é…
        if segment["speaker"] == clip_info["speaker"]:
            # æ£€æŸ¥æ—¶é—´æ˜¯å¦åŒ¹é…ï¼ˆå…è®¸æ›´å¤§è¯¯å·®ï¼‰
            if (segment["start"] == clip_info["start"]) and (segment["end"] == clip_info["end"]):
                return segment
    return None


def process_clips(clips_dir, diarization_file, model, language=None):
    """å¤„ç†æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µå¹¶æ›´æ–°è¯†åˆ«ç»“æœ"""
    # è¯»å–è¯´è¯äººåˆ†ç¦»ç»“æœ
    with open(diarization_file, 'r', encoding='utf-8') as f:
        segments = json.load(f)
    
    # æ¸…ç©ºæ‰€æœ‰ç‰‡æ®µçš„raw_textã€result_textå­—æ®µï¼Œç¡®ä¿å®Œå…¨é‡æ–°è¯†åˆ«
    for segment in segments:
        segment.pop('raw_text', None)
        segment.pop('result_text', None)
    
    print(f"Loaded {len(segments)} segments")
    
    # å¤„ç†æ¯ä¸ªè¯´è¯äººçš„ç‰‡æ®µ
    speaker_dirs = [d for d in Path(clips_dir).iterdir() if d.is_dir()]
    
    # ç”¨äºè·Ÿè¸ªå¤„ç†çŠ¶æ€
    total_processed = 0
    total_errors = 0
    
    for speaker_dir in speaker_dirs:
        speaker_name = speaker_dir.name
        print(f"\nProcessing segments for {speaker_name}...")
        
        # è·å–è¯¥è¯´è¯äººçš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
        audio_files = list(speaker_dir.glob("*.wav"))
        print(f"Found {len(audio_files)} audio files")
        
        # ä¸ºæ¯ä¸ªè¯´è¯äººåˆ›å»ºå•ç‹¬çš„è¿›åº¦æ¡
        pbar = tqdm(audio_files, desc=f"è¯†åˆ« {speaker_name}")
        
        for audio_file in pbar:
            try:
                # ä»æ–‡ä»¶åæå–ä¿¡æ¯
                clip_info = get_clip_info_from_filename(audio_file.name)
                if not clip_info:
                    print(f"Cannot parse filename: {audio_file.name}")
                    total_errors += 1
                    continue
                
                # åœ¨segmentsä¸­æ‰¾åˆ°åŒ¹é…çš„ç‰‡æ®µ
                matching_segment = find_matching_segment(segments, clip_info)
                if not matching_segment:
                    print(f"No matching segment found: {audio_file.name}")
                    total_errors += 1
                    continue
                
                # è¿›è¡Œè¯­éŸ³è¯†åˆ«
                print(f"\nRecognizing: {audio_file.name}")
                transcription = transcribe_with_faster_whisper(str(audio_file), model, language)
                
                # ä¿å­˜è¯†åˆ«ç»“æœ
                matching_segment['raw_text'] = transcription
                print(f"Transcription result: {transcription}")
                total_processed += 1
                
            except Exception as e:
                print(f"Error processing file {audio_file.name}: {str(e)}")
                total_errors += 1
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶è€Œä¸æ˜¯ä¸­æ–­æ•´ä¸ªè¿‡ç¨‹
                continue
        
        # æ›´æ–°è¿›åº¦æ¡æè¿°ä¿¡æ¯
        pbar.set_postfix({"å·²å¤„ç†": total_processed, "é”™è¯¯": total_errors})
    
    # è¿‡æ»¤æ‰raw_textä¸ºç©ºçš„æ¡ç›®
    filtered_segments = [segment for segment in segments if segment.get('raw_text', '').strip()]
    removed_count = len(segments) - len(filtered_segments)
    
    # ä¿å­˜æ›´æ–°åçš„ç»“æœ
    with open(diarization_file, 'w', encoding='utf-8') as f:
        json.dump(filtered_segments, f, ensure_ascii=False, indent=2)
    
    print(f"\nProcessing completion statistics:")
    print(f"- Total processed: {total_processed} files")
    print(f"- Errors: {total_errors} files")
    print(f"- Removed empty entries: {removed_count}")
    print(f"- Final saved: {len(filtered_segments)} entries")
    
    if total_errors > 0:
        print(f"\nWarning: {total_errors} files failed to process, please check the error messages above")
    
    print(f"\nAll recognition results saved to: {diarization_file}")


def main():
    # å¯¼å…¥torchä»¥æ£€æŸ¥CUDAå¯ç”¨æ€§
    try:
        import torch
        cuda_available = torch.cuda.is_available()
    except ImportError:
        cuda_available = False
    
    parser = argparse.ArgumentParser(description="ASRå¤„ç†è„šæœ¬")
    parser.add_argument("--model_size", type=str, default="large-v3", help="Whisperæ¨¡å‹å¤§å°")
    parser.add_argument("--device", type=str, default="cuda" if cuda_available else "cpu", help="è¿è¡Œè®¾å¤‡")
    parser.add_argument("--compute_type", type=str, default="float16" if cuda_available else "int8", 
                       help="è®¡ç®—ç±»å‹")
    parser.add_argument("--language", type=str, default=None, help="éŸ³é¢‘è¯­è¨€")
    
    args = parser.parse_args()
    
    # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
    os.makedirs(MODEL_DIR, exist_ok=True)
    
    # æ£€æŸ¥CUDAå…¼å®¹æ€§ï¼Œå¦‚æœCUDAç‰ˆæœ¬ä¸åŒ¹é…åˆ™ä½¿ç”¨CPU
    if args.device == "cuda":
        try:
            # å°è¯•åˆå§‹åŒ–ä¸€ä¸ªç®€å•çš„CUDAæ“ä½œæ¥æ£€æŸ¥å…¼å®¹æ€§
            import torch
            if torch.cuda.is_available():
                test_tensor = torch.zeros(1).cuda()
                print("CUDA environment check passed")
            else:
                print("CUDA unavailable, will use CPU")
                args.device = "cpu"
                args.compute_type = "int8"
        except Exception as e:
            print(f"CUDA environment check failed: {e}")
            print("Will use CPU for inference")
            args.device = "cpu"
            args.compute_type = "int8"
    
    # åˆå§‹åŒ–Faster-Whisperæ¨¡å‹
    print("Loading Faster-Whisper model...")
    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨æ¨¡å‹models--Systran--faster-whisper-large-v3
    model_path = f"{MODEL_DIR}"
    print(f"Checking model path: {os.path.join(model_path, f'models--Systran--faster-whisper-{args.model_size}')}")
    if os.path.exists(os.path.join(model_path, f"models--Systran--faster-whisper-{args.model_size}")):
        print("Found local model directory")
        # æŸ¥æ‰¾snapshotsç›®å½•ä¸­çš„å®é™…æ¨¡å‹ç‰ˆæœ¬
        snapshots_dir = os.path.join(model_path, f"models--Systran--faster-whisper-{args.model_size}", "snapshots")
        print(f"Checking snapshots directory: {snapshots_dir}")
        if os.path.exists(snapshots_dir):
            print("Found snapshots directory")
            # è·å–ç¬¬ä¸€ä¸ªå¿«ç…§ç›®å½•ï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªï¼‰
            snapshot_dirs = os.listdir(snapshots_dir)
            print(f"Snapshot directory list: {snapshot_dirs}")
            if snapshot_dirs:
                actual_model_path = os.path.join(snapshots_dir, snapshot_dirs[0])
                print(f"Using local model: {actual_model_path}")
                try:
                    model = WhisperModel(actual_model_path, device=args.device, compute_type=args.compute_type)
                except Exception as e:
                    print(f"GPU model loading failed: {e}")
                    print("Attempting to load model with CPU...")
                    # åœ¨è¿™é‡Œä¹Ÿç¡®ä¿ä½¿ç”¨CPUè®¾å¤‡å’Œå…¼å®¹çš„è®¡ç®—ç±»å‹
                    args.device = "cpu"
                    args.compute_type = "int8"
                    model = WhisperModel(actual_model_path, device=args.device, compute_type=args.compute_type)
            else:
                print("No model snapshot directory found, re-downloading from HuggingFace...")
                # æ·»åŠ é‡è¯•æœºåˆ¶
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        print(f"Attempt {attempt + 1} to download Faster-Whisper model...")
                        model = WhisperModel(args.model_size, device=args.device, compute_type=args.compute_type, download_root=model_path)
                        print("Model download completed!")
                        break
                    except Exception as e:
                        print(f"[âš ï¸ Warning] Attempt {attempt + 1} to download Faster-Whisper model failed: {str(e)}")
                        if attempt < max_retries - 1:
                            print(f"[ğŸ”„ Retry] Waiting 5 seconds before attempt {attempt + 2}...")
                            time.sleep(5)
                        else:
                            raise
        else:
            print("Snapshots directory not found, re-downloading from HuggingFace...")
            # æ·»åŠ é‡è¯•æœºåˆ¶
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    print(f"Attempt {attempt + 1} to download Faster-Whisper model...")
                    model = WhisperModel(args.model_size, device=args.device, compute_type=args.compute_type, download_root=model_path)
                    print("Model download completed!")
                    break
                except Exception as e:
                    print(f"[Warning] Attempt {attempt + 1} to download Faster-Whisper model failed: {str(e)}")
                    if attempt < max_retries - 1:
                        print(f"[Retry] Waiting 5 seconds before attempt {attempt + 2}...")
                        time.sleep(5)
                    else:
                        raise
    else:
        # å¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä»HuggingFaceä¸‹è½½å¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•
        print(f"Model not found locally, will download from HuggingFace to: {model_path}")
        # æ·»åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"Attempt {attempt + 1} to download Faster-Whisper model...")
                model = WhisperModel(args.model_size, device=args.device, compute_type=args.compute_type, download_root=model_path)
                print("Model download completed!")
                break
            except Exception as e:
                print(f"[âš ï¸ Warning] Attempt {attempt + 1} to download Faster-Whisper model failed: {str(e)}")
                if "cublas64_12.dll" in str(e):
                    print("Detected CUDA library issue, switching to CPU mode")
                    args.device = "cpu"
                    args.compute_type = "int8"
                    try:
                        model = WhisperModel(args.model_size, device=args.device, compute_type=args.compute_type, download_root=model_path)
                        print("Model loaded successfully in CPU mode!")
                        break
                    except Exception as cpu_e:
                        print(f"Failed to load model in CPU mode as well: {str(cpu_e)}")
                        if attempt < max_retries - 1:
                            print(f"[ğŸ”„ Retry] Waiting 5 seconds before attempt {attempt + 2}...")
                            time.sleep(5)
                        else:
                            raise
                elif attempt < max_retries - 1:
                    print(f"[ğŸ”„ Retry] Waiting 5 seconds before attempt {attempt + 2}...")
                    time.sleep(5)
                else:
                    raise
    print("Model loading completed!")
    
    # è®¾ç½®é»˜è®¤è·¯å¾„ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰
    clips_dir = os.path.join(project_root, "temp", "clips")
    diarization_file = os.path.join(project_root, "results", "speaker_diarization.json")
    
    # å¤„ç†éŸ³é¢‘ç‰‡æ®µ
    process_clips(clips_dir, diarization_file, model, args.language)


if __name__ == "__main__":
    main()